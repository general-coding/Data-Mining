{
 "metadata": {
  "name": "",
  "signature": "sha256:78be90940cfdf84023e0a52bc30f01c3d432027d5ce839e4fae0516a101baa9d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CSE 4334/5334 Programming Assignment 1 (P1)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Spring 2015"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Due: 11:59pm Central Time, Monday, February 23, 2015"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this assignment, you will implement a toy \"search engine\". More specifically, you code will read a corpus and produce TF-IDF vectors for documents in the corpus. Then, given a query string, you code will compute the cosine similarity between the query vector and the document vectors and return the document that gets the highest similarity score. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Requirements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This assignment must be done individually. You must implement the whole assignment by yourself. Academic dishonety will have serious consequences.\n",
      "* You are encouraged to discuss the assignment with other students, but you are not allowed to disclose your solution. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use a corpus of all the State of the Union addresses from 1790 to 2015, originally obtained from http://stateoftheunion.onetwothree.net/index.shtml. We processed the corpus and provide you a .zip file, which includes 229 .txt files. Each of the 229 files contains the text of a speech and is named by the president's name and the date of the speech. The .zip file can be downloaded from Blackboard (\"Course Materials\" > \"Programming Assignment 1\" > \"Attached Files: stateoftheunionaddresses.zip\")."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Programming Language"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. You are required to use Python 3.4.1. You are required to submit a single .py file of your code. We will test your code under the particular version of Python 3.4.1. So make sure you develop your code using the same version. \n",
      "\n",
      "2. You are expected to use several modules in NLTK--a natural language processing toolkit for Python. NLTK doesn't come with Python by default. You need to install it and \"import\" it in your .py file. \n",
      "\n",
      "3. In programming assignment 1, other than NLTK, you are not allowed to use any other non-standard Python package. However, you are free to use anything from the the Python Standard Library that comes with Python 3.4.1 (https://docs.python.org/3/library/).\n",
      "\n",
      "4. NLTK's website (http://www.nltk.org/index.html) provides a lot of useful information, including a book http://www.nltk.org/book/, as well as installation instructions (http://www.nltk.org/install.html). (However, if you are less experienced with Python, we recommend you not to install NLTK and other packages individually. See below.) \n",
      "\n",
      "5. In future programming assignments you will use some other non-standard packages. You may choose to install NLTK and other packages individually, if you feel confident in doing so. We recommend you use Anacoda, which is a Python distribution with close to 200 non-standard packages (including NLTK), a GUI IDE called Spyder, IPython Notebook which is necessary for viewing this document, and many other nice things. To get Anacode, follow the link at https://store.continuum.io/cshop/anaconda/ and make sure you choose the installer for Python 3.4."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tasks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You code should accomplish the following tasks:\n",
      "\n",
      "(1) <b>Read</b> the 229 .txt files, each of which has the content of a State of the Union speech. The following code does it. Make sure to replace the directory by your directory where the files are stored."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "corpus_root = 'C:\\\\Users\\\\chengkai\\\\Desktop\\\\stateoftheunionaddresses'\n",
      "for filename in os.listdir(corpus_root):\n",
      "    file = open(os.path.join(corpus_root, filename), \"r\")\n",
      "    doc = file.read()\n",
      "    file.close() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(2) <b>Tokenize</b> the content of each file. For this, you need a tokenizer. For example, the following piece of code uses a regular expression tokenizer to return all course numbers in a string. Play with it and edit it. You can change the regular expression and the string to observe different output results. \n",
      "\n",
      "For tokenizing the State of the Union speeches, let's all use RegexpTokenizer(r'[a-zA-Z]+'). What tokens will it produce? What limitations does it have? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "tokenizer = RegexpTokenizer(r'[A-Z]{2,3}[1-9][0-9]{3,3}')\n",
      "tokens = tokenizer.tokenize(\"CSE4334 and CSE5334 are taught together. IE3013 is an undergraduate course.\")\n",
      "print(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['CSE4334', 'CSE5334', 'IE3013']\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(3) Perform <b>stopword removal</b> on the obtained tokens. NLTK already comes with a stopword list, as a corpus in the \"NLTK Data\" (http://www.nltk.org/nltk_data/). You need to install this corpus. Follow the instructions at http://www.nltk.org/data.html. You can also find the instruction in this book: http://www.nltk.org/book/ch01.html (Section 1.2 Getting Started with NLTK). Basically, use the following statements in Python interpreter. A pop-up window will appear. Click \"Corpora\" and choose \"stopwords\" from the list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After the stopword list is downloaded, you will find a file \"english\" in folder nltk_data/corpora/stopwords, where folder nltk_data is the download directory in the step above. The file contains 127 stopwords. nltk.corpus.stopwords will give you this list of stopwords. Try the following piece of code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "print(stopwords.words('english'))\n",
      "print(sorted(stopwords.words('english')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
        "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'my', 'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 's', 'same', 'she', 'should', 'so', 'some', 'such', 't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(4) Also perform <b>stemming</b> on the obtained tokens. NLTK comes with a Porter stemmer. Try the following code and learn how to use the stemmer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.porter import PorterStemmer\n",
      "stemmer = PorterStemmer()\n",
      "stemmer.stem('studying')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(5) Using the tokens, compute the <b>TF-IDF vector</b> for each document. Use the following equation that we learned in the lectures to calculate the term weights:  $$(1+log_{10}{tf_{t,d}})\\times(log_{10}{\\frac{N}{df_t}}).$$ Note that the TF-IDF vectors should be normalized (i.e., their lengths should be 1). \n",
      "\n",
      "Represent a TF-IDF vector by a dictionary. The following is a sample TF-IDF vector."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "{'ga': 0.03199783386584945, 'full': 0.0018049599463918031, 'american': 0.0021521564085256223, 'attract': 0.014556280630063255}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "{'american': 0.0021521564085256223,\n",
        " 'full': 0.0018049599463918031,\n",
        " 'attract': 0.014556280630063255,\n",
        " 'ga': 0.03199783386584945}"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(6) Given a query string, calculate the query vector. Compute the <b>cosine similarity</b> between the query vector and every document vector. Return the document that attains the highest cosine similarity score."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What to Submit "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Submit through Blackboard your source code in a single .py file. You can use any standard Python library. The only non-standard library/package allowed for this assignment is NLTK.\n",
      "\n",
      "The file must define at least the following functions:\n",
      "\n",
      "* query(qstring): return the document that has the highest similarity score with respect to 'qstring'.\n",
      "\n",
      "* gettfidfvec(filename): return the TF-IDF vector for a file (speech), given the file name.\n",
      "\n",
      "* getidf(token): return the inverse document frequency of a token. If the token doesn't exist in the corpus, return 0. \n",
      "\n",
      "* docdocsim(filename1,filename2): return the cosine similarity betwen two speeches (files).\n",
      "\n",
      "* querydocsim(qstring,filename): return the cosine similairty between a query string and a document. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some sample results that we should expect from a correct implementation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* print(query(\"health insurance wall street\"))\n",
      "\n",
      "Barack ObamaFebruary 24, 2009.txt\n",
      "\n",
      "* print(gettfidfvec(\"Barack ObamaJanuary 20, 2015.txt\"))\n",
      "\n",
      "{'gtmo': 0.06409239784086913, 'back': 0.014138622561019694, 'east': 0.009917077437312133, 'california': 0.01843040866461211, ...}  (Note that the elements in a dictionary are not ordered. They may be displayed in arbitrarily different orders across diffrent runs.)\n",
      " \n",
      "* print(getidf(\"health\"))\n",
      "\n",
      "0.16671088398542636\n",
      "\n",
      "* print(docdocsim(\"Barack ObamaJanuary 20, 2015.txt\", \"Barack ObamaJanuary 28, 2014.txt\"))\n",
      "\n",
      "0.2841737078448495\n",
      "\n",
      "* print(querydocsim(\"health insurance wall street\", \"Barack ObamaJanuary 28, 2014.txt\"))\n",
      "\n",
      "0.033325932927668986"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will evaluate your code by calling the functions specificed above. So, make sure to use the same function names, parameter names/types/orders as specified above. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}